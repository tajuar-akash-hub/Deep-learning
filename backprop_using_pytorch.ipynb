{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tajuar-akash-hub/Deep-learning/blob/main/backprop_using_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Dataset (Pure PyTorch)\n",
        "# ----------------------------\n",
        "# Format: [Study Hours, Sleep Hours, HSC marks]\n",
        "import torch\n",
        "\n",
        "data = torch.tensor([\n",
        "    [2., 3., 3.],\n",
        "    [5., 6., 4.],\n",
        "    [7., 8., 5.]\n",
        "])\n",
        "\n",
        "X_all = data[:, 0:2] # Features\n",
        "y_all = data[:, 2]   # Target\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Initialize Parameters\n",
        "# ----------------------------\n",
        "def initialize_parameters(layer_dims):\n",
        "    torch.manual_seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        # We use .clone() and * 0.1 to match your specific manual init logic\n",
        "        parameters['W' + str(l)] = torch.ones((layer_dims[l-1], layer_dims[l])) * 0.1\n",
        "        parameters['b' + str(l)] = torch.zeros((layer_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Forward Pass Logic\n",
        "# ----------------------------\n",
        "def linear_forward(A_prev, W, b):\n",
        "    # W.T @ A_prev matches your np.dot(W.T, A_prev)\n",
        "    Z = torch.matmul(W.T, A_prev) + b\n",
        "    return Z\n",
        "\n",
        "def L_layer_forward(X, parameters):\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L + 1):\n",
        "        A_prev = A\n",
        "        Wl = parameters['W' + str(l)]\n",
        "        bl = parameters['b' + str(l)]\n",
        "        A = linear_forward(A_prev, Wl, bl)\n",
        "\n",
        "    # Returning A (y_hat) and A_prev (A1) specifically for your update function\n",
        "    return A, A_prev\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 4. Manual Update Logic\n",
        "# ----------------------------------------------------\n",
        "def update_parameters(parameters, y, y_hat, A1, X, lr=0.001):\n",
        "    # Your manual backprop logic preserved exactly\n",
        "    # error = 2 * (y - y_hat)\n",
        "    err_signal = 2 * (y - y_hat)\n",
        "\n",
        "     #  Save OLD W2 values before updating\n",
        "\n",
        "    W2_00_old = parameters['W2'][0, 0].item()\n",
        "    W2_10_old = parameters['W2'][1, 0].item()\n",
        "\n",
        "    # Update Layer 2\n",
        "    parameters['W2'][0, 0] += lr * err_signal * A1[0, 0]\n",
        "    parameters['W2'][1, 0] += lr * err_signal * A1[1, 0]\n",
        "    parameters['b2'][0, 0] += lr * err_signal # Preserving your specific b2 logic\n",
        "\n",
        "    # Update Layer 1 (Using current W2 as per your original code)\n",
        "    parameters['W1'][0, 0] += lr * err_signal * parameters['W2'][0, 0] * X[0, 0]\n",
        "    parameters['W1'][0, 1] += lr * err_signal * parameters['W2'][0, 0] * X[1, 0]\n",
        "    parameters['b1'][0, 0] += lr * err_signal * parameters['W2'][0, 0]\n",
        "\n",
        "    parameters['W1'][1, 0] += lr * err_signal * parameters['W2'][1, 0] * X[0, 0]\n",
        "    parameters['W1'][1, 1] += lr * err_signal * parameters['W2'][1, 0] * X[1, 0]\n",
        "    parameters['b1'][1, 0] += lr * err_signal * parameters['W2'][1, 0]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Training Loop\n",
        "# ----------------------------\n",
        "params = initialize_parameters([2, 2, 1])\n",
        "epochs = 5\n",
        "\n",
        "for i in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for j in range(X_all.shape[0]):\n",
        "        # Prepare sample\n",
        "        X_sample = X_all[j].reshape(2, 1)\n",
        "        y_sample = y_all[j]\n",
        "\n",
        "        # Forward\n",
        "        y_hat, A1 = L_layer_forward(X_sample, params)\n",
        "        y_hat_scalar = y_hat[0, 0]\n",
        "\n",
        "        # Update\n",
        "        params = update_parameters(params, y_sample, y_hat_scalar, A1, X_sample)\n",
        "\n",
        "        # Loss calculation\n",
        "        loss = (y_sample - y_hat_scalar) ** 2\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch - {i+1} Loss - {epoch_loss / X_all.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chifTO16VoHs",
        "outputId": "fef9b3be-054e-4107-ec1c-a7b2d606d471"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch - 1 Loss - 14.67866039276123\n",
            "Epoch - 2 Loss - 13.701451301574707\n",
            "Epoch - 3 Loss - 12.494118690490723\n",
            "Epoch - 4 Loss - 11.040447076161703\n",
            "Epoch - 5 Loss - 9.366330782572428\n"
          ]
        }
      ]
    }
  ]
}